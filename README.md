# RAG API - Retrieval-Augmented Generation System

A production-ready RAG (Retrieval-Augmented Generation) API built with FastAPI, ChromaDB, and Google's Gemini AI. This system enables semantic search and question-answering over uploaded documents.

## ğŸ—ï¸ Architecture

```
User Question
    â†“
FastAPI Endpoint
    â†“
Document Processing
    â”œâ”€â”€ Text Extraction (PDF/TXT)
    â”œâ”€â”€ Text Chunking (with overlap)
    â””â”€â”€ Embedding Generation (Gemini)
    â†“
ChromaDB Vector Store
    â†“
Semantic Search (cosine similarity)
    â†“
Context Retrieval (Top-K chunks)
    â†“
LLM Answer Generation (Gemini)
    â†“
Structured Response
```

## âœ¨ Features

- **Document Upload**: Support for PDF and TXT files
- **Intelligent Chunking**: Sentence-aware chunking with configurable overlap
- **Semantic Search**: Vector similarity search using Gemini embeddings
- **Controlled Generation**: LLM constrained to use only retrieved context
- **Clean Architecture**: Separation of concerns with service layer pattern
- **Type Safety**: Full Pydantic model validation
- **Error Handling**: Comprehensive error handling and validation

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- Google Gemini API Key

### Installation

1. **Clone the repository**
```bash
git clone <your-repo-url>
cd rag-api
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure environment**
```bash
cp .env.example .env
# Edit .env and add your GEMINI_API_KEY
```

5. **Run the application**
```bash
uvicorn app.main:app --reload
```

The API will be available at `http://localhost:8000`

## ğŸ“š API Documentation

Once running, visit:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Endpoints

#### 1. Health Check
```bash
GET /health
```

#### 2. Upload Document
```bash
POST /documents/upload
Content-Type: multipart/form-data

# Example with curl
curl -X POST "http://localhost:8000/documents/upload" \
  -F "file=@document.pdf"
```

**Response:**
```json
{
  "document_id": "uuid-here",
  "filename": "document.pdf",
  "chunks_created": 15,
  "message": "Document uploaded and processed successfully"
}
```

#### 3. Query Documents
```bash
POST /documents/query
Content-Type: application/json

# Example with curl
curl -X POST "http://localhost:8000/documents/query" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is the main topic of the document?",
    "document_id": "optional-uuid-to-filter"
  }'
```

**Response:**
```json
{
  "answer": "Generated answer based on context...",
  "source_chunks": [
    {
      "chunk_id": "doc-id_chunk_0",
      "similarity_score": 0.95,
      "text": "Relevant text chunk..."
    }
  ],
  "document_ids": ["doc-id"]
}
```

## ğŸ›ï¸ Project Structure

```
rag-api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI application & endpoints
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py       # Pydantic models
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ document_service.py   # Main orchestration service
â”‚   â”‚   â”œâ”€â”€ embedding_service.py  # Gemini embedding operations
â”‚   â”‚   â”œâ”€â”€ vector_store.py       # ChromaDB interface
â”‚   â”‚   â””â”€â”€ llm_service.py        # Gemini LLM operations
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ chunker.py       # Text chunking logic
â”‚       â””â”€â”€ text_extractor.py # PDF/TXT extraction
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸ¯ Design Principles

### 1. **Separation of Concerns**
- **Services**: Business logic separated into focused services
- **Utils**: Reusable utilities for text processing
- **Models**: Type-safe data models with Pydantic

### 2. **Dependency Injection**
- Settings injected via FastAPI dependencies
- Services initialized with required dependencies
- Easy to test and mock

### 3. **Error Handling**
- Comprehensive validation at each layer
- Meaningful error messages
- HTTP status codes follow REST conventions

### 4. **Configuration Management**
- Environment-based configuration
- Sensible defaults with override capability
- Cached settings for performance

### 5. **Clean Code**
- Type hints throughout
- Docstrings for all public methods
- Consistent naming conventions
- Single Responsibility Principle

## âš™ï¸ Configuration

Edit `.env` file to customize:

```env
# API Configuration
GEMINI_API_KEY=your_key_here

# Chunking Parameters
CHUNK_SIZE=600          # Target chunk size in tokens
CHUNK_OVERLAP=100       # Overlap between chunks

# Retrieval Parameters
TOP_K=5                 # Number of chunks to retrieve

# Storage
CHROMADB_PATH=./chroma_db
```

## ğŸ§ª Testing the System

### Test with a Sample Document

1. **Upload a document**:
```bash
curl -X POST "http://localhost:8000/documents/upload" \
  -F "file=@sample.pdf"
```

Save the `document_id` from response.

2. **Ask questions**:
```bash
curl -X POST "http://localhost:8000/documents/query" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What are the key findings?",
    "document_id": "your-document-id"
  }'
```

3. **Test out-of-scope questions**:
Questions not answerable from the document should return:
```
"I don't know based on the provided context."
```

## ğŸ” How It Works

### Document Processing Pipeline

1. **Text Extraction**: Extracts text from PDF/TXT files
2. **Chunking**: Splits text into overlapping chunks (~600 tokens each)
3. **Embedding**: Generates vector embeddings using Gemini
4. **Storage**: Stores chunks and embeddings in ChromaDB

### Query Pipeline

1. **Query Embedding**: Converts user question to embedding
2. **Semantic Search**: Finds top-K similar chunks via cosine similarity
3. **Context Assembly**: Combines retrieved chunks as context
4. **Answer Generation**: Gemini generates answer constrained to context
5. **Response**: Returns answer with source attribution

## ğŸš¨ Common Issues

### API Key Not Set
```
âš ï¸  WARNING: GEMINI_API_KEY not configured
```
**Solution**: Add your API key to `.env` file

### Import Errors
**Solution**: Ensure virtual environment is activated and dependencies installed

### ChromaDB Permission Errors
**Solution**: Ensure write permissions in project directory

## ğŸ“ Key Evaluation Points

âœ… Document upload works  
âœ… Text extraction handles PDF and TXT  
âœ… Chunking with proper overlap  
âœ… Embeddings stored in ChromaDB  
âœ… Semantic search retrieves relevant chunks  
âœ… LLM constrained to retrieved context  
âœ… Out-of-scope questions handled correctly  
âœ… Clean code with proper structure  
âœ… Type safety with Pydantic  
âœ… Comprehensive error handling  

## ğŸ“ Learning Outcomes

This project demonstrates:
- **RAG architecture**: How retrieval enhances generation
- **Vector databases**: Semantic search with embeddings
- **API design**: RESTful principles with FastAPI
- **Clean architecture**: Maintainable, testable code structure
- **LLM integration**: Controlled generation with external knowledge

## ğŸ“„ License

This is an educational project for internship purposes.

## ğŸ¤ Contributing

This is a one-day icebreaker project. Focus on understanding each component before making modifications.
